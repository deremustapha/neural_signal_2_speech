# PhonemeDecoding

End-to-end pipeline for decoding **phonemes** and **acoustic features** from surface EMG.  
This repo builds on excellent prior work from:

- [dgaddy/silent_speech](https://github.com/dgaddy/silent_speech)  
- [tbenst/silent_speech](https://github.com/tbenst/silent_speech?tab=readme-ov-file)  

Much credit to them‚Äîthis project would not exist without their contributions.

---

## üìÇ Repository Contents

- `read_emg.py` ‚Äî dataset & preprocessing (EMG/audio/phonemes)
- `data_utils.py` ‚Äî feature extraction, normalizers, phoneme inventory
- `align.py` ‚Äî DTW alignment utilities
- `models.py` ‚Äî VQ-VAE tokenizer + EMG‚Üíphoneme model with Mixture-of-Recursions 
- `tokenizer.py` ‚Äî trains and exports an EMG VQ-VAE tokenizer
- `phoneme_decoding.py` ‚Äî main training loop with logging, checkpoints, metrics
- `utils.py` ‚Äî tokenizer export/load + DTW loss functions

---

## üì• Data Setup

1. **Download Digital Voicing dataset**  
   From [Zenodo](https://zenodo.org/records/4064409).


   
2. **Get text alignments + files from `dgaddy/silent_speech`**  
Download:
- ` TextGrid alignments`  
- `testset_largedev.json`  
- `normalizers.pkl`  



3. **Set paths in code**  
- In `read_emg.py`, configure  (`--silent_data_directories`, `--voiced_data_directories`, `--testset_file`, `--text_align_directory`)
- In `data_utils.py`, set `--normalizers_file` path to `normalizers.pkl`

---

## ‚öôÔ∏è Installation

1. **Create Conda environment**
We provide an `environment.txt` file to reproduce the exact conda environment.

   ```bash
   conda create -n brain2speech --file environment.txt
   conda activate brain2speech


## üöÄ Training Steps

### 1. Train the tokenizer

Train a VQ-VAE tokenizer on raw EMG:

```bash
python tokenizer.py


### 2. Train the phoneme decoder

EMG into phonemes:

```bash
python phoneme_decoding.py


### 3. Listen to generated audio from BMISEMG2SPEECH Dataset
<div style="display:flex; gap:16px; flex-wrap:wrap;">
  <figure style="margin:0">
    <figcaption>Example 1</figcaption>
    <audio controls src="audio/example1.mp3"></audio>
  </figure>
  <figure style="margin:0">
    <figcaption>Example 2</figcaption>
    <audio controls src="https://github.com/deremustapha/neural_signal_2_speech/blob/master/audio_demo/Text_0.wav"></audio>
  </figure>
</div>
